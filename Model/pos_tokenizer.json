{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 25449, "word_counts": "{\"n\": 8918, \"v\": 15932, \"aj\": 361, \"pd\": 194, \"av\": 40, \"c\": 4}", "word_docs": "{\"n\": 8918, \"v\": 15932, \"aj\": 361, \"pd\": 194, \"av\": 40, \"c\": 4}", "index_docs": "{\"2\": 8918, \"1\": 15932, \"3\": 361, \"4\": 194, \"5\": 40, \"6\": 4}", "index_word": "{\"1\": \"v\", \"2\": \"n\", \"3\": \"aj\", \"4\": \"pd\", \"5\": \"av\", \"6\": \"c\"}", "word_index": "{\"v\": 1, \"n\": 2, \"aj\": 3, \"pd\": 4, \"av\": 5, \"c\": 6}"}}